{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70d8b6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e5bc6012",
   "metadata": {},
   "outputs": [],
   "source": [
    "#packages \n",
    "#Load packages :pytorch, sklearn etc.,\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import decomposition\n",
    "from sklearn import manifold\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import copy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0dccbf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading in the .npz file to train the model \n",
    "def get_training_data(load=False, datafile ='test.npz'):\n",
    "    \n",
    "    if not load:\n",
    "        # This grabs the training data files from the appropriate directory\n",
    "        mypath = r'C:\\Users\\Sam\\Documents\\PSU\\DS 440\\DS440-CAPSTONE-main\\test.npz'  #change path to fit your path file\n",
    "        training_data_files = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "        \n",
    "        training_t = []  \n",
    "        training_f = []\n",
    "        training_class = []\n",
    "        training_info = []\n",
    "        for f in training_data_files:\n",
    "            data = np.load(mypath+f)\n",
    "            all_time=data['all_time']\n",
    "            all_flux=data['all_flux']\n",
    "            ra = data['ra']\n",
    "            dec = data['dec']\n",
    "            injmu_percentile=data['injmu_percentile']\n",
    "            injduration=data['injduration']\n",
    "            injloc_x=-1 * data['injloc_x']\n",
    "            injloc_y=-1 * data['injloc_y']\n",
    "            injpeak=data['injpeak']\n",
    "            \n",
    "            info = ra,dec,injmu_percentile,injduration,injloc_x,injloc_y,injpeak #injduration, injpeak\n",
    "            for i, t in enumerate(all_time):\n",
    "                f = all_flux[i]\n",
    "                row = np.floor(i/9) - 4\n",
    "                col = i%9 - 4\n",
    "                if np.all(np.isnan(f)):\n",
    "                    continue\n",
    "                if np.sqrt((row - injloc_x)**2 + (col-injloc_y)**2)<=2:\n",
    "                    training_t.append(t)\n",
    "                    training_f.append(f)\n",
    "                    training_class.append(1)\n",
    "                    training_info.append(info)\n",
    "                    \n",
    "                elif np.sqrt((row - injloc_x)**2 + (col-injloc_y)**2)>5:\n",
    "                    training_t.append(t)\n",
    "                    training_f.append(f)\n",
    "                    training_class.append(0)\n",
    "                    training_info.append(info)\n",
    "        np.savez(datafile,training_t = training_t, training_f = training_f,\n",
    "                training_class = training_class, training_info = training_info)\n",
    "        return training_t, training_f,training_class,training_info\n",
    "    else:\n",
    "        data = np.load(datafile, allow_pickle=True)\n",
    "        training_t = data['training_t']         # needs to be padded and then shuffled\n",
    "        training_f = data['training_f']         \n",
    "        training_class = data['training_class']  \n",
    "        training_info = data['training_info']   \n",
    "        return training_t, training_f,training_class,training_info\n",
    "training_t, training_f,training_class,training_info = get_training_data(load=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b83232f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#padding the arrays to match the max size\n",
    "longest_len = 3658\n",
    "for i in np.arange(len(training_t)):\n",
    "    mylen = len(training_t[i])\n",
    "    if mylen>longest_len:\n",
    "        longest_len = mylen\n",
    "new_training_t = np.zeros((len(training_t),longest_len))\n",
    "new_training_f = np.zeros((len(training_t),longest_len))\n",
    "for i in np.arange(len(training_t)):\n",
    "    nl = len(training_t[i])\n",
    "    before_N=int(longest_len/2-(nl/2))\n",
    "    new_training_t[i,before_N:nl+before_N] = training_t[i]   # Data to be used\n",
    "    new_training_f[i,before_N:nl+before_N] = training_f[i]   # Data to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7fe636-9e81-41b2-ace5-fc231f5d141a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07fb2923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(new_training_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0caf7ac-3204-4804-9fe9-61729a57d0d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_training_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e26bd12-274c-4e0b-8dab-b3f35f2c0db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12116, 3658)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_training_f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "acc47e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12116,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_class.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "01ed611e-31f2-435b-9209-405fbbe39d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(data.Dataset):\n",
    "    def __init__(self, training_class, new_training_f, transform=None, target_transform=None):\n",
    "        self.labels = training_class\n",
    "        self.data = new_training_f\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #img_path = os.path.join(self.data, self.labels.iloc[idx, 0])\n",
    "        curve = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            curve = self.transform(curve)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return curve, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c7a159b7-2c9e-4754-a145-4ef551eda926",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = CustomDataset(training_class, new_training_f, transform=None, target_transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "89bc90d3-72e6-4c33-86ab-bc989fe478c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_RATIO = 0.9\n",
    "TEST_RATIO = 0.66\n",
    "\n",
    "n_train_examples = int(len(train) * TEST_RATIO)\n",
    "n_test_examples = len(train) - n_train_examples\n",
    "n_train_examples = int(n_train_examples * VALID_RATIO)\n",
    "n_valid_examples = len(train) - n_test_examples - n_train_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5cb3ab38-280c-4757-a983-1a4758422f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, valid_data = data.random_split(train, [n_train_examples, n_test_examples, n_valid_examples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "98fc496a-f821-432d-8469-797d287ec922",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_iterator = data.DataLoader(train_data, shuffle=True, batch_size=BATCH_SIZE)\n",
    "\n",
    "valid_iterator = data.DataLoader(valid_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_iterator = data.DataLoader(test_data, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182dbe58-5824-4f99-8a25-828230d63888",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "544a9108-de41-44fe-813c-d6d2e5be951b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_fc = nn.Linear(input_dim, 1400)\n",
    "        self.hidden_fc1 = nn.Linear(1400, 530)\n",
    "        self.hidden_fc2 = nn.Linear(530, 200)\n",
    "        self.hidden_fc3 = nn.Linear(200, 80)\n",
    "        self.output_fc = nn.Linear(80, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # x = [batch size, height, width]\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        x = x.view(batch_size, -1)\n",
    "\n",
    "        # x = [batch size, height * width]\n",
    "\n",
    "        h_1 = F.relu(self.input_fc(x))\n",
    "\n",
    "        # h_1 = [batch size, 1400]\n",
    "\n",
    "        h_2 = F.relu(self.hidden_fc1(h_1))\n",
    "\n",
    "        # h_2 = [batch size, 530]\n",
    "        \n",
    "        h_3 = F.relu(self.hidden_fc2(h_2))\n",
    "\n",
    "        # h_3 = [batch size, 200]\n",
    "        \n",
    "        h_4 = F.relu(self.hidden_fc3(h_3))\n",
    "\n",
    "        # h_4 = [batch size, 80]\n",
    "\n",
    "        y_pred = self.output_fc(h_4)\n",
    "\n",
    "        # y_pred = [batch size, output dim]\n",
    "\n",
    "        return y_pred, h_4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e045d9-aa16-4623-bf36-96ca5942e29d",
   "metadata": {},
   "source": [
    "define our model by creating an instance of it and setting the correct input and output dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "12d6a268-497b-4362-950d-26cd4b931048",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 3658\n",
    "OUTPUT_DIM = 2\n",
    "\n",
    "model = MLP(INPUT_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81952984-46f0-4b22-bbec-b95c432e6bb0",
   "metadata": {},
   "source": [
    "calculate the number of trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b0bfc3f5-e7ee-4d23-b2f1-834125f25f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bca8750-ab78-4b4a-8039-11c6a7f4c6b1",
   "metadata": {},
   "source": [
    "The first layer has 3658 neurons connected to 1400 neurons, so 3658*1400 weighted connections plus 1400 bias terms.\n",
    "\n",
    "The second layer has 1400 neurons connected to 530 neurons, 1400*530 weighted connections plus 530 bias terms.\n",
    "\n",
    "The third layer has 530 neurons connected to 200 neurons, 530*200 weighted connections plus 200 bias terms.\n",
    "\n",
    "The fourth layer has 200 neurons connected to 80 neurons, 200*80 weighted connections plus 80 bias terms.\n",
    "\n",
    "The fifth layer has 80 neurons connected to 2 neurons, 80*2 weighted connections plus 2 bias terms.\n",
    "\n",
    "$$3658 \\cdot 1400 + 1400 + 1400 \\cdot 530 + 530 + 530 \\cdot 200 + 200 + 200 \\cdot 80 + 80 + 80 \\cdot 2 + 2 = 5,987,572 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "63d259c6-a378-4aa2-a8eb-0716784043cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 5,987,572 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1cd5c6d0-dd38-4bd5-af6f-a497eee62197",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "16f4f196-0463-4fd2-a9fb-cb6d01fd6124",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c0ba512e-83a8-40e6-9e1d-6a279fda9d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fc8a6f3d-8501-42bb-964c-909b2757101f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "14b0b449-acaa-4cbd-8aa4-19851bb4ca92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_pred, y):\n",
    "    top_pred = y_pred.argmax(1, keepdim=True)\n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "    acc = correct.float() / y.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f3a611fe-69a7-4383-b54a-56ba5b942aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, device):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for (x, y) in tqdm(iterator, desc=\"Training\", leave=False):\n",
    "\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred, _ = model(x)\n",
    "\n",
    "        loss = criterion(y_pred, y)\n",
    "\n",
    "        acc = calculate_accuracy(y_pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f4551b11-d59f-43d6-93a3-927ce116ec09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, device):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for (x, y) in tqdm(iterator, desc=\"Evaluating\", leave=False):\n",
    "\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            y_pred, _ = model(x)\n",
    "\n",
    "            loss = criterion(y_pred, y)\n",
    "\n",
    "            acc = calculate_accuracy(y_pred, y)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "afb090a9-8c92-47c7-96c8-76a7ec70bd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d37cd409-4a3a-466c-b646-5a07af3a02fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a72366a245c42e6bc58d555b268fcab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc849dceb17f44fd809b8089424fd519",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Float but found Double",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-8706c7dcb380>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-63-90ccdaba7bd2>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, iterator, optimizer, criterion, device)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-54-11a46b0464ce>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m# x = [batch size, height * width]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mh_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_fc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m# h_1 = [batch size, 1400]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expected scalar type Float but found Double"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in trange(EPOCHS):\n",
    "\n",
    "    start_time = time.monotonic()\n",
    "\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, device)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, device)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "\n",
    "    end_time = time.monotonic()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c732a4-55f5-4710-9470-2cd2bed22d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('tut1-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216b40eb-a94a-499c-ab74-a848aad91265",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0700760-5c9b-43e4-83d3-45f762f91b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
